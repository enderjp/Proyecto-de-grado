# -*- coding: utf-8 -*-
"""gridsearchCV_ETNIA.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kULoeYn5QcVgbRf1kl1Y5znZnISTgSXx
"""

import pandas as pd
import numpy as np

!pip freeze # Ver librerias instaladas

from sklearn import svm, datasets
from sklearn.model_selection import GridSearchCV
from sklearn.utils import parallel_backend
from sklearn.pipeline import Pipeline
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import f_classif
from time import time
from sklearn.preprocessing import MultiLabelBinarizer

data=pd.read_csv('/content/drive/MyDrive/features_final.csv',header=0)

#---Preprocesamiento---#


# Balancear las clases
# Unir clases 3 y 4
#data=data['raza']
for i in range(len(data)):
    if data.loc[i,'raza']==4:
       data.loc[i,'raza']=3

 # Categorizar edades       
for i in range(len(data)):
    if ((data.loc[i,'edad'] >=0) & (data.loc[i,'edad'] <=10)):
        data.loc[i,'edad']=0
    elif ((data.loc[i,'edad'] >=11) & (data.loc[i,'edad'] <=20)):
        data.loc[i,'edad']=1
    elif ((data.loc[i,'edad'] >=21) & (data.loc[i,'edad'] <=35)):
        data.loc[i,'edad']=2
    elif ((data.loc[i,'edad'] >=36) & (data.loc[i,'edad'] <=50)):
        data.loc[i,'edad']=3 
    elif ((data.loc[i,'edad'] >=51) & (data.loc[i,'edad'] <=65)):
          data.loc[i,'edad']=4 
    else:
        data.loc[i,'edad']=5

data['edad'].value_counts()

data['raza'].value_counts()

#Verificar porcentaje de las clases

cauc=0
black=0
asian=0
#indian=0
others=0
data=data_balanced['raza']

# Contar cuÃ¡ntas imagenes hay de cada raza
for i in range(len(data)):
    if data[i]==0:
        cauc+=1
    elif data[i]==1:
        black+=1
    elif data[i]==2:
        asian+=1
    else:
        others+=1
        
# Determinar el porcentaje      
c= (cauc/len(data)) *100 
b = (black/len(data)) *100 
a = (asian/len(data)) *100
#i= (indian/len(data)) *100
o =  (others/len(data)) *100

print("Porcentaje de razas en el UTK dataset:")
print("CaucÃ¡sico: %.2f " % c)
print("Negro: %.2f" % b)
print("AsiÃ¡tico: %.2f" % a)
#print("Indio: %.2f" % i)
print("Indio/Hispano/M. Oriente: %.2f" % o)
print(c+b+a+o)

X= data.drop(['genero','raza','edad'],axis=1)
Y= data['raza']

from sklearn.model_selection import train_test_split
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import f_classif

from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import StratifiedKFold
from sklearn.feature_selection import RFECV
from sklearn.svm import SVC, LinearSVC
from sklearn.model_selection import GridSearchCV
from sklearn.pipeline import Pipeline
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.linear_model import Perceptron
from sklearn.kernel_approximation import Nystroem
from sklearn.preprocessing import StandardScaler, PowerTransformer

print(X.shape)
print(Y.shape)

#Separar data de entrenamiento y validación

#from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2,random_state=50,shuffle=True) #200

# Validación cruzada de 5 carpetas
#cv=StratifiedKFold(5, shuffle=True, random_state=200)

##-------Parámetros para el entrenamiento----#
from sklearn.multiclass import OneVsRestClassifier
from sklearn.linear_model import SGDClassifier
from sklearn.linear_model import Perceptron
# Validación cruzada de 5 carpetas
cv=StratifiedKFold(5, shuffle=True, random_state=200)

#-----Estimador/modelo-----#

model = SVC(kernel='rbf',verbose=4) 


# Selección de características
fs = SelectKBest(score_func=f_classif) # ANOVA o f-test

# Método de aproximación de Kernel Nystroem
k=1392#int(len(X[1])/5) # número de características escogido para aproxar el kernel
#Ny = Nystroem(kernel='poly',random_state=1,n_components=k,degree=2)

#----Definir el papeline a evaluar-------------#
# feature extraction + kernel aproximation + training
pipeline = Pipeline(steps=[('anova',fs),
                          ('transformer',PowerTransformer()),
                           ('estimador', model)],verbose=3)

# Parámetros a evaluar
parameters = { 'anova__k':[1392],
              'estimador__C':[1,10,100],
             'estimador__gamma':[0.0001,0.001,0.005]}


#Búsqueda de parámetros
search = GridSearchCV(pipeline, param_grid=parameters,cv=cv,scoring='accuracy', verbose=10)

tiempo_inicial=time()
#with parallel_backend('spark'):
search.fit(X_train, y_train)
    
tiempo_final=time()    
#X_new = search.fit_transform(X)
#Mostrar resultados
print('Best Mean Accuracy-score: %.3f' % search.best_score_)
print('Best Config: %s' % search.best_params_)

tiempo_ejecucion = tiempo_final - tiempo_inicial
print('Tiempo de ejecución', tiempo_ejecucion)
resumen= pd.DataFrame(search.cv_results_)

# Guardar resumen
resumen.to_csv('raza_definitivo1.csv',index=False)

# Matriz de confusión
y_pred=search.best_estimator_.predict(X_test)
from sklearn.metrics import confusion_matrix
print (confusion_matrix(y_test, y_pred))

#Import scikit-learn metrics module for accuracy calculation
from sklearn import metrics

# Model Accuracy: how often is the classifier correct?
print("Accuracy:",metrics.accuracy_score(y_test, y_pred))

# Reporte de clasificación
from sklearn.metrics import classification_report
# PARA UTK
summary= classification_report(y_test, y_pred, target_names=['Caucásico',
                                                             'Africano/Af.americano',
                                                             'Asiático','Latino/M.oriente/Indio'])

print(summary)

#Graficar AUC

from sklearn.metrics import plot_roc_curve
import matplotlib.pyplot as plt


svc_disp = plot_roc_curve(search.best_estimator_, X_test, y_test)
rfc_disp = plot_roc_curve(rfc, X_test, y_test, ax=svc_disp.ax_)

plt.show()

from sklearn.metrics import classification_report
# PARA UTK


print(summary)

[[825  28  43 120]
 [ 39 757  19  74]
 [ 42  14 558  52]
 [143  77  34 900]]
Accuracy: 0.8161073825503355
                           precision    recall  f1-score   support

                Caucásico       0.79      0.81      0.80      1016
                 Africano       0.86      0.85      0.86       889
                 Asiático       0.85      0.84      0.85       666
Indio/Hispano/M. Oriente.       0.79      0.78      0.78      1154

                 accuracy                           0.82      3725
                macro avg       0.82      0.82      0.82      3725
             weighted avg       0.82      0.82      0.82      3725


# svc weight balanced con toda la data

precision    recall  f1-score   support

        0-10       0.94      0.89      0.91       650
       11-20       0.58      0.31      0.40       308
       21-35       0.71      0.90      0.79      2031
       36-50       0.51      0.36      0.42       828
       51-65       0.55      0.51      0.53       591
         66+       0.72      0.60      0.65       331

    accuracy                           0.69      4739
   macro avg       0.67      0.59      0.62      4739
weighted avg       0.68      0.69      0.68      4739

# Entrenamiento del modelo final (a toda la data)
from sklearn.multiclass import OneVsRestClassifier
# Estimador con los mejores parámetros
estimador = SVC(kernel='rbf',verbose=3,C=10,gamma=0.001) 

# ANOVA
fs = SelectKBest(f_classif,k=1392) 

#------Aproximación de kernel --------------#
#Ny = Nystroem(kernel='rbf',random_state=1,n_components=k)
#---- Definir el papeline ------------#
# feature extraction + kernel aproximation + training
modelo_final_etnia = Pipeline(steps=[('anova',fs),
                          ('transformer',PowerTransformer()),
                         #  ('nystroem',Ny), 
                           ('estimador', estimador],verbose=4)
modelo_final_etnia.fit(X_train,y_train)

print("Entrenamiento final completado")

# Guardar modelo
import joblib 
filename = 'modelo_final_etnia.joblib'
joblib.dump(modelo_final_etnia, filename,compress=3)

# Cargar el modelo entrenado y hacer predicciones
loaded_model = joblib.load('/content/modelo_final_etnia.joblib')

tiempo_inicial=time()
result = modelo_final_etnia.score(X_test,y_test)
tiempo_final=time()
t=tiempo_final-tiempo_inicial
print("Accuracy",result)
print('Tiempo de ejecución', t)



# Lerning curve ALL data with CV

from sklearn.model_selection import learning_curve

train_sizes, train_scores, test_scores = learning_curve(model, 
                                                        X, 
                                                        Y,
                                                        # Number of folds in cross-validation
                                                        cv=cv,
                                                        # Evaluation metric
                                                        scoring='accuracy',
                                                        # Use all computer cores
                                                       # n_jobs=-1, 
                                                        # different sizes of the training set
                                                        train_sizes=np.linspace(0.01, 1, 10),verbose=4,
                                                        shuffle=True,
                                                        random_state=20)

# Create means and standard deviations of training set scores
train_mean = 1*np.mean(train_scores, axis=1)
train_std = 1*np.std(train_scores, axis=1)

# Create means and standard deviations of test set scores
test_mean =1*np.mean(test_scores, axis=1)
test_std = 1*np.std(test_scores, axis=1)

# Draw lines
plt.plot(train_sizes, train_mean, '--', color="r",  label="Training score")
plt.plot(train_sizes, test_mean, color="g", label="Cross-validation score")

# Draw bands
plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, color="#DDDDDD")
plt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, color="#DDDD")

# Create plot
plt.title("Learning Curve. HOG+LBP. LFW dataset. Linear kernel.")
plt.xlabel("Training Set Size"), plt.ylabel("Accuracy Score"), plt.legend(loc="best")
plt.tight_layout()
plt.show()


##############################